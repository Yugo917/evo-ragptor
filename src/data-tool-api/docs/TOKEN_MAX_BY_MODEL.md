## üìä **Tableau r√©capitulatif ‚Äî Mod√®les vs max tokens (contexte)**

| Mod√®le / API                        | Type         | Fen√™tre max (tokens)  | Notes principales                    |
| ----------------------------------- | ------------ | --------------------- | ------------------------------------ |
| **GPT-3.5 Turbo** (`gpt-3.5-turbo`) | OpenAI       | 16 385                | Tr√®s utilis√© pour chat & embeddings  |
| **GPT-4 Turbo** (`gpt-4-turbo`)     | OpenAI       | 128 000               | Ultra grande fen√™tre, tarif r√©duit   |
| **Claude 3 Opus**                   | Anthropic    | 200 000               | Excellente compr√©hension longue      |
| **Claude 3 Sonnet / Haiku**         | Anthropic    | 200 000               | Moins cher, m√™me contexte            |
| **Mistral 7B**                      | Open source  | 8 192                 | Mod√®le dense tr√®s performant         |
| **Mixtral 8x7B** (MoE)              | Open source  | 32 768                | MoE performant, co√ªt optimis√©        |
| **Gemini 1.5 Pro**                  | Google       | 1 000 000 (streaming) | Ultra large, mais acc√®s limit√©       |
| **LLaMA 2 (7B/13B/65B)**            | Open source  | 4 096                 | Standard chez Meta                   |
| **LLaMA 3 (8B/70B)**                | Open source  | 8 192                 | Plus grand contexte, plus efficace   |
| **Command R / R+ (Reka)**           | Open source  | 128 000               | Optimis√© pour RAG                    |
| **TinyLLaMA 1.1B**                  | Open source  | 2 048                 | Ultra l√©ger, peu de contexte         |
| **Phi-2 (1.5B)**                    | Microsoft    | 2 048                 | Tr√®s compact, rapide √† fine-tuner    |
| **BGE (embedding models)**          | Hugging Face | 512 ‚Äì 8192            | `bge-large-en-v1.5` = 512 tokens max |
| **text-embedding-3-large**          | OpenAI       | 8 192                 | Embedding performant multi-langue    |
| **text-embedding-ada-002**          | OpenAI       | 8 192                 | Embedding rapide et pas cher         |

---

## üìå Notes techniques

* üß† **Fen√™tre de contexte = limite de tokens** que le mod√®le peut traiter **en une requ√™te**.
* ‚öôÔ∏è **Tokens ‚â† mots** : 1 token ‚âà 4 caract√®res en anglais, 0.75 mot en moyenne.
* üîÅ Pour les **mod√®les d‚Äôembedding**, la fen√™tre max d√©termine la **taille du chunk que tu peux vectoriser**.

---

## üìä **Mod√®les LLM utilisables localement ‚Äî Fen√™tre de contexte**

| Mod√®le                      | Taille (param√®tres) | Fen√™tre max (tokens) | Type d‚Äôusage principal            | Notes techniques                            |
| --------------------------- | ------------------- | -------------------- | --------------------------------- | ------------------------------------------- |
| **TinyLLaMA**               | 1.1B                | 2 048                | Chat l√©ger / embedded LLM         | Parfait en CPU / edge                       |
| **Phi-2**                   | 1.5B                | 2 048                | Bon pour dev, raisonnant bien     | Moins bon en g√©n√©ration longue              |
| **LLaMA 2 (7B)**            | 7B                  | 4 096                | G√©n√©ration g√©n√©rale               | Qualit√© correcte, vite obsol√®te             |
| **LLaMA 3 (8B)**            | 8B                  | 8 192                | Chat + t√¢che g√©n√©rale             | üî• Nouveau standard local                   |
| **Mistral 7B**              | 7B                  | 8 192                | Mod√®le dense, rapide              | Excellent en CPU/GPU local                  |
| **Mixtral 8x7B (MoE)**      | 12.9B (active)      | 32 768               | Performant et √©conome (2 experts) | Context ultra-large                         |
| **OpenChat 3.5 / 4**        | 7B ‚Äì 8B             | 8 192                | Style GPT, dialogue/chat          | Tr√®s fluide, entra√Æn√© sur partages sociaux  |
| **Command R / R+ (Reka)**   | \~35B eq. (MoE)     | 128 000              | üîç Optimis√© RAG et retrieval      | Disponible via vLLM, pas encore dans Ollama |
| **Nous Hermes 2 (LLaMA 3)** | 7B ‚Äì 8B             | 8 192                | Chat / g√©n√©raliste                | Fine-tuned roleplay et multit√¢che           |
| **Code LLaMA 7B / 13B**     | 7B ‚Äì 13B            | 16 384               | G√©n√©ration de code                | Tr√®s fort pour devs, en local via llama.cpp |
| **WizardLM 2 / Dolphin**    | 7B ‚Äì 13B            | 8 192                | Chat affin√© style assistant       | Tr√®s appr√©ci√© en local                      |
| **BGE Small / Base**        | 110M ‚Äì 1.5B         | 512 ‚Äì 8192           | Embedding uniquement              | Parfait pour vector store local             |

---

## üß† Recommandations par cas d‚Äôusage local

| Cas d‚Äôusage                              | Mod√®les recommand√©s                     |
| ---------------------------------------- | --------------------------------------- |
| Chat local rapide (CPU)                  | TinyLLaMA, Phi-2, Mistral 7B            |
| Chat local de qualit√© (GPU)              | LLaMA 3 (8B), Mixtral, Nous Hermes 2    |
| Embedding local                          | BGE Small / Large, E5, Instructor XL    |
| Chat multi-docs avec long contexte (RAG) | Mixtral, Command R+, LLaMA 3 fine-tun√©  |
| G√©n√©ration de code locale                | Code LLaMA, Deepseek Coder, WizardCoder |
